{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7071255,"sourceType":"datasetVersion","datasetId":4072234},{"sourceId":7192095,"sourceType":"datasetVersion","datasetId":4158851}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seaborn\n!pip install transformers\n!pip install imblearn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Download datasets","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/read-books-mla2/Train.csv',\n                       usecols = ['review_id', 'rating', 'review_text'])\ntrain_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/read-books-mla2/Test.csv'\n                     , usecols = ['review_id', 'review_text'])\ntest_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_ids = test_df.review_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Investigate train dataset a bit","metadata":{}},{"cell_type":"markdown","source":"From Dataset description we know that:\n\n**user_id** - Id of user<br/>\n**book_id** - Id of Book<br/>\n**review_id** - Id of review<br/>\n**rating** - rating from 0 to 5<br/>\n**review_text** - review text<br/>\n**date_added** - date added<br/>\n**date_updated** - date updated<br/>\n**read_at** - read at<br/>\n**started_at** - started at<br/>\n**n_votes** - no. of votes<br/>\n**n_comments** - no. of comments<br/>","metadata":{}},{"cell_type":"code","source":"print(f'Len of train dataset: {len(train_df)}')\nprint(f'Len of test dataset: {len(test_df)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = train_df.rating)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Clear the data","metadata":{}},{"cell_type":"code","source":"y = train_df.pop('rating')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_df = pd.concat([train_df.review_text, test_df.review_text]).reset_index(drop = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to lower\ndef to_lower(text):\n    return text.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove stopwords\n#from nltk.corpus import stopwords\n#stop = set(stopwords.words('english'))\nstop = {'a', 'the', 'www', 'http', 'https', 'com'}\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word not in stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom urllib.parse import urlparse\n\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+')\n    return url.sub(r'',text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nremove = string.punctuation\n\nperiod = '.'\nremove = remove.replace(period, '')\n\ndef remove_punctuation(text):\n    pattern = re.compile(r\"[{}]\".format(re.escape(remove)))\n    \n    res = []\n    for word in text.split():\n        \n        # remove all punctiations except periods\n        new_word = pattern.sub(r' ', word)\n        new_word = new_word.strip(period)\n        \n        try:\n            float(new_word)\n        except:\n            new_word = new_word.replace(period, ' ')\n            \n        res.append(new_word)\n    \n    return ' '.join(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all non-alphabetic chars (punctuation, numbers, emojies ...)\ndef remove_non_alphabetic(text):\n    alpha = re.compile(r'[^a-zA-Z]') \n    return alpha.sub(r' ', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_quotes(text):\n    alpha = re.compile(r'\"([^\"\\\\]|\\\\.)*\"') \n    return alpha.sub(r' ', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_spoiler_alert(text):\n    spoiler = re.compile(r'(\\(view spoiler\\).*?\\(hide spoiler\\))') \n    return spoiler.sub(r' ', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_small_words(text):\n    return ' '.join([word for word in text.split() if len(word) > 2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# after applying usual text preprocessing steps (list below)\n# I found ot that many reviews are empty! (because not all of them are meaningful ðŸ¤·)\n# so I commented almost all of these steps out to avoid empty reviews in both train and test sets\n\ndef process_text(text):\n    text = to_lower(text)\n    #text = remove_url(text)\n    text = remove_punctuation(text)\n    #text = remove_spoiler_alert(text)\n    #text = remove_quotes(text)\n    #text = remove_non_alphabetic(text)\n    text = remove_stopwords(text)\n    #text = remove_small_words(text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_df = common_df.apply(process_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check min number of words\ncommon_df.apply(lambda x : len(x.split())).min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('common_df.pkl', 'wb') as f:\n    pickle.dump(common_df, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('common_df.pkl', 'rb') as f:\n    common_df = pickle.load(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_df[12]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. TPU setup","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# try:\n#    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n# except ValueError:\ntpu = None\ngpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \nif tpu:\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU')\nelif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    strategy = tf.distribute.get_strategy() \n    print('Running on single GPU ', gpus[0].name)\nelse:\n    strategy = tf.distribute.get_strategy() \n    print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. XLM_RoBERTa model","metadata":{}},{"cell_type":"code","source":"X_train = common_df.iloc[:len(y)]\nX_test = common_df.iloc[len(y):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ny_cat = to_categorical(y)\ny_cat.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModel, AutoTokenizer\n\nMAX_LEN = 100 # use only first 100 words of review to predict the rating\nmodel_name = 'xlm-roberta-base'\n\n# tokenizing\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntrain_inputs = tokenizer(X_train.to_list(), \n                         max_length = MAX_LEN,\n                         padding = 'max_length', \n                         truncation = True,\n                         return_tensors = 'np')\n\ntest_inputs = tokenizer(X_test.to_list(), \n                        max_length = MAX_LEN,\n                        padding = 'max_length',\n                        truncation = True,\n                        return_tensors = 'np')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.savez('train_test_inputs.npz', train_inputs=train_inputs, test_inputs=test_inputs)\nwith open('inputs.pkl', 'wb') as file:\n    pickle.dump({'train_inputs': train_inputs, 'test_inputs': test_inputs}, file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fine-tune the model\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\nwith strategy.scope():\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels = y_cat.shape[-1])\n    model.compile(Adam(learning_rate = 5e-5), \n                  loss = 'categorical_crossentropy', \n                  metrics=['accuracy'], steps_per_execution = 200)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\n\n# build the model\nwith strategy.scope():\n    encoder = TFAutoModel.from_pretrained(model_name)\n    \n    input_word_ids = Input(shape = (MAX_LEN, ), dtype = tf.int32, name = \"input_ids\")\n    input_mask = Input(shape = (MAX_LEN, ), dtype = tf.int32, name = \"attention_mask\") \n\n    embedding = encoder([input_word_ids, input_mask])[1] # pooled_output\n    x = Dropout(0.3)(embedding)\n    x = Dense(128, activation = 'relu', kernel_regularizer = regularizers.L2(0.1))(x)\n    x = Dropout(0.3)(x)\n    x = Dense(32, activation = 'relu', kernel_regularizer = regularizers.L2(0.1))(x)\n    x = Dropout(0.3)(x)\n    x = Dense(y_cat.shape[1], activation = 'softmax')(x)\n    \n    model = Model(inputs = [input_word_ids, input_mask], \n                  outputs = x)\n    \n    model.compile(Adam(learning_rate = 1e-5), \n                  loss = 'categorical_crossentropy', \n                  metrics=['accuracy'], steps_per_execution = 200)\n    \nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nearly_stop = EarlyStopping(patience = 5, restore_best_weights = True, verbose = 1) # val_loss\nreduce_lr = ReduceLROnPlateau(factor = 0.1, patience = 2, mode = 'min', verbose = 1) # val_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fine-tune the model\n\n# https://github.com/huggingface/transformers/issues/20709\n# The problem is that Keras recognizes dict objects \n# but not our BatchEncoding returned by the tokenizer, \n# even though BatchEncoding is a subclass of dict.\n# If you replace the last line with model.fit(dict(tokenized_data), labels) it should work.\n\nhistory = model.fit(dict(train_inputs), \n                    y_cat,\n                    epochs = 50,\n                    verbose = 1,\n                    validation_split = 0.1,\n                    batch_size = 64 * strategy.num_replicas_in_sync,\n                    callbacks = [reduce_lr, early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = history.history['loss']\nacc = history.history['accuracy']\nval_loss = history.history['val_loss']\nval_acc = history.history['val_accuracy']\n\nepochs = range(1, len(loss) + 1)\n\nplt.figure(figsize=(16, 5))\n#accuracy\nplt.subplot(1,2,1)\nplt.plot(epochs, acc, 'bo', label = 'Training accuracy')\nplt.plot(epochs, val_acc, 'r', label = 'Validation accuracy')\nplt.legend()\n\n#loss\nplt.subplot(1,2,2)\nplt.plot(epochs, loss, 'bo', label = 'Trainig loss')\nplt.plot(epochs, val_loss, 'r', label = 'Validation loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub['review_id'] = test_df_ids\nsub['rating'] = [np.argmax(i) for i in model.predict(dict(test_inputs))]\nsub.head()\nsub.to_csv(\"submission2.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}